{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkokati/Edu-Commenter/blob/main/ExplainableTranslator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJyGnJv3_Pir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8f55e9-7dfa-4902-f6fc-14089b166da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement colab_tunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for colab_tunnel\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask torch transformers sentencepiece accelerate spacy colab_tunnel Flask-Cors\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "zrv9tT4eYopr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344b5024-b785-4b1f-c724-af74a1d8227a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Flask-Cors"
      ],
      "metadata": {
        "id": "kmfxx-cyak4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8530374-2c90-44de-890f-8c361cdc331d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Flask-Cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from Flask-Cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from Flask-Cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->Flask-Cors) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: Flask-Cors\n",
            "Successfully installed Flask-Cors-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# immersive id=\"explainable_translator_flask_v1\" type=\"code\" title=\"Advanced Rule-Based Explainable Translator (Flask App)\"\n",
        "# app.py\n",
        "# Flask application for an explainable translator (EN<>FR)\n",
        "# with advanced rule-based linguistic explanation and a professional UI.\n",
        "# Designed to be runnable in a Google Colab notebook.\n",
        "\n",
        "# --- Imports ---\n",
        "print(\"Importing libraries...\")\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import gc # Garbage Collector\n",
        "from typing import List, Tuple, Dict, Optional, Any\n",
        "\n",
        "# Set environment variable to avoid matplotlib warning in some environments\n",
        "# Create a dummy config directory if it doesn't exist, useful in some minimal environments\n",
        "config_dir = os.path.join(os.getcwd(), \"configs_translator\")\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "os.environ['MPLCONFIGDIR'] = config_dir\n",
        "\n",
        "# Flag to track if essential libraries are successfully imported\n",
        "essential_libraries_loaded = True\n",
        "\n",
        "# Ensure necessary libraries are installed (for Colab)\n",
        "try:\n",
        "    import flask\n",
        "    import torch\n",
        "    import transformers\n",
        "    import sentencepiece # Required by some transformers models\n",
        "    import accelerate # Improves performance with transformers\n",
        "    import spacy\n",
        "    # Import pyngrok for tunneling\n",
        "    import pyngrok\n",
        "\n",
        "    # Check if spaCy models are installed, download if not\n",
        "    try:\n",
        "        en_model = spacy.load(\"en_core_web_sm\")\n",
        "        fr_model = spacy.load(\"fr_core_news_sm\")\n",
        "        print(\"spaCy models already installed.\")\n",
        "    except OSError:\n",
        "        print(\"Downloading spaCy models...\")\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "        os.system(\"python -m spacy download fr_core_news_sm\")\n",
        "        try:\n",
        "            en_model = spacy.load(\"en_core_web_sm\")\n",
        "            fr_model = spacy.load(\"fr_core_news_sm\")\n",
        "            print(\"spaCy models downloaded and loaded.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Failed to load spaCy models even after attempting download: {e}\")\n",
        "             essential_libraries_loaded = False\n",
        "\n",
        "\n",
        "    print(\"Required libraries (Flask, Torch, Transformers, spaCy, pyngrok) imported successfully.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing required libraries: {e}\")\n",
        "    print(\"Please ensure all necessary libraries are installed by running:\")\n",
        "    # Updated installation command to use pyngrok\n",
        "    print(\"!pip install Flask torch transformers sentencepiece accelerate spacy pyngrok Flask-Cors\")\n",
        "    print(\"Then run the spaCy model downloads:\")\n",
        "    print(\"!python -m spacy download en_core_web_sm\")\n",
        "    print(\"!python -m spacy download fr_core_news_sm\")\n",
        "    essential_libraries_loaded = False # Set flag to False if imports fail\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during imports or spaCy loading: {e}\")\n",
        "    essential_libraries_loaded = False # Set flag to False on other errors\n",
        "\n",
        "\n",
        "# Load spaCy models globally if imports were successful\n",
        "if essential_libraries_loaded:\n",
        "    try:\n",
        "        nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "        nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load spaCy models after initial check: {e}\")\n",
        "        nlp_en = None\n",
        "        nlp_fr = None\n",
        "        essential_libraries_loaded = False # Update flag if spaCy loading fails here\n",
        "\n",
        "\n",
        "# Flask setup\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "# Using render_template_string to embed HTML directly for Colab simplicity.\n",
        "# For a production app, use render_template and a 'templates' folder.\n",
        "\n",
        "# Optional: Enable CORS if frontend is served from a different origin (common in Colab)\n",
        "try:\n",
        "    # Only attempt to import CORS if Flask is available\n",
        "    if 'flask' in sys.modules:\n",
        "        from flask_cors import CORS\n",
        "        cors_available = True\n",
        "    else:\n",
        "        cors_available = False\n",
        "except ImportError:\n",
        "    print(\"Flask-Cors not found. Install with 'pip install Flask-Cors' if needed for cross-origin requests.\")\n",
        "    cors_available = False\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "if cors_available:\n",
        "    CORS(app) # Enable CORS for all routes\n",
        "\n",
        "\n",
        "print(\"Flask app initialized.\")\n",
        "\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "\n",
        "# Model identifiers from Hugging Face Hub\n",
        "MODEL_EN_FR = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "MODEL_FR_EN = \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "\n",
        "# --- Global Variables for Models & Tokenizers ---\n",
        "# Use a dictionary to cache loaded models/tokenizers\n",
        "loaded_models = {}\n",
        "device_global = None\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def clean_memory():\n",
        "    \"\"\"Force garbage collection and clear CUDA cache if available.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    # print(\"Memory cleaned.\") # Optional debug message\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Gets the appropriate torch device.\"\"\"\n",
        "    global device_global\n",
        "    if device_global is None:\n",
        "        # Only attempt to use CUDA/MPS if torch is available\n",
        "        if 'torch' in sys.modules:\n",
        "            if torch.cuda.is_available():\n",
        "                device_global = torch.device(\"cuda\")\n",
        "            # Check for MPS (Apple Silicon) - uncomment if needed and PyTorch supports your MPS version\n",
        "            # elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "            #     device_global = torch.device(\"mps\")\n",
        "            else:\n",
        "                device_global = torch.device(\"cpu\")\n",
        "        else:\n",
        "            device_global = torch.device(\"cpu\") # Default to CPU if torch not available\n",
        "        print(f\"Using device: {device_global}\")\n",
        "    return device_global\n",
        "\n",
        "def load_model_and_tokenizer(model_name: str):\n",
        "    \"\"\"Loads a pre-trained model and tokenizer from Hugging Face, caching them.\"\"\"\n",
        "    global loaded_models\n",
        "    # Only attempt to load models if transformers and torch are available\n",
        "    if not ('transformers' in sys.modules and 'torch' in sys.modules):\n",
        "        raise RuntimeError(\"Translation libraries (transformers/torch) not loaded. Cannot load translation model.\")\n",
        "\n",
        "    device = get_device()\n",
        "\n",
        "    if model_name not in loaded_models:\n",
        "        print(f\"Loading model and tokenizer for: {model_name}...\")\n",
        "        try:\n",
        "            # Use from_pretrained with torch_dtype=torch.float16 for potential memory savings\n",
        "            # This requires accelerate >= 0.20.0 and a supported GPU\n",
        "            # Added error handling for model loading\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            model.eval() # Set to evaluation mode\n",
        "            loaded_models[model_name] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "            print(f\"{model_name} loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_name}: {e}\")\n",
        "            # Re-raise the error to be caught by the calling function\n",
        "            raise RuntimeError(f\"Failed to load model: {model_name}. Check model name, internet connection, and available resources.\") from e\n",
        "    else:\n",
        "        # Ensure model is on the correct device if device changed (unlikely in a simple script)\n",
        "        loaded_models[model_name][\"model\"].to(device)\n",
        "        print(f\"Using cached model and tokenizer for: {model_name}\")\n",
        "\n",
        "    return loaded_models[model_name][\"model\"], loaded_models[model_name][\"tokenizer\"]\n",
        "\n",
        "# --- Linguistic Analysis and Explanation Logic (Highly Advanced Rule-Based) ---\n",
        "\n",
        "def analyze_sentence(text: str, lang: str) -> Optional[spacy.tokens.Doc]:\n",
        "    \"\"\"Analyzes a sentence using spaCy.\"\"\"\n",
        "    # Only attempt analysis if spaCy models are loaded\n",
        "    if not ('spacy' in sys.modules and nlp_en and nlp_fr):\n",
        "         return None\n",
        "\n",
        "    if lang == \"en\" and nlp_en:\n",
        "        return nlp_en(text)\n",
        "    elif lang == \"fr\" and nlp_fr:\n",
        "        return nlp_fr(text)\n",
        "    else:\n",
        "        print(f\"spaCy model for language '{lang}' not loaded or spaCy not available.\")\n",
        "        return None\n",
        "\n",
        "def get_morph_features(token: spacy.tokens.Token) -> Dict[str, str]:\n",
        "    \"\"\"Converts spaCy token morphology to a readable dictionary.\"\"\"\n",
        "    return {key: str(value) for key, value in token.morph.to_dict().items()}\n",
        "\n",
        "def describe_verb_form_rulebased(token: spacy.tokens.Token, lang: str) -> str:\n",
        "    \"\"\"Describes a verb's form in user-friendly terms based on morphology.\"\"\"\n",
        "    morph = get_morph_features(token)\n",
        "    description_parts = []\n",
        "\n",
        "    # Tense\n",
        "    tense = morph.get('Tense')\n",
        "    if tense and tense != 'N/A':\n",
        "        description_parts.append(f\"in the **{tense}** tense\")\n",
        "\n",
        "    # Mood\n",
        "    mood = morph.get('Mood')\n",
        "    if mood and mood != 'N/A':\n",
        "         description_parts.append(f\"in the **{mood}** mood\")\n",
        "\n",
        "    # Person and Number (often go together for agreement)\n",
        "    person = morph.get('Person')\n",
        "    number = morph.get('Number')\n",
        "    if person and number and person != 'N/A' and number != 'N/A':\n",
        "        if lang == 'fr':\n",
        "            description_parts.append(f\"conjugated for the **{person} person {number}**\")\n",
        "        else:\n",
        "            description_parts.append(f\"is in the **{person} person {number}** form\")\n",
        "    elif person and person != 'N/A':\n",
        "         description_parts.append(f\"is conjugated for the **{person} person**\")\n",
        "    elif number and number != 'N/A':\n",
        "         description_parts.append(f\"is in the **{number}** number\")\n",
        "\n",
        "    # Verb Form (e.g., infinitive, participle, finite)\n",
        "    verb_form = morph.get('VerbForm')\n",
        "    if verb_form and verb_form != 'N/A':\n",
        "         description_parts.append(f\"({verb_form} form)\")\n",
        "\n",
        "    # Aspect\n",
        "    aspect = morph.get('Aspect')\n",
        "    if aspect and aspect != 'N/A':\n",
        "         description_parts.append(f\"with '{aspect}' aspect\")\n",
        "\n",
        "\n",
        "    if not description_parts:\n",
        "        return \"in an unspecified form\"\n",
        "\n",
        "    return \", \".join(description_parts)\n",
        "\n",
        "def describe_noun_features_rulebased(token: spacy.tokens.Token, lang: str) -> List[str]:\n",
        "    \"\"\"Describes noun features in user-friendly terms, especially for French.\"\"\"\n",
        "    descriptions = []\n",
        "    morph = get_morph_features(token)\n",
        "\n",
        "    if lang == 'fr':\n",
        "        # French Noun Gender\n",
        "        gender = morph.get(\"Gender\")\n",
        "        if gender and isinstance(gender, str):\n",
        "             descriptions.append(f\"This French noun is typically **{gender.lower()}**.\")\n",
        "\n",
        "        # French Noun Number\n",
        "        number = morph.get(\"Number\")\n",
        "        if number and isinstance(number, str):\n",
        "             descriptions.append(f\"It is in the **{number}** number.\")\n",
        "\n",
        "    elif lang == 'en':\n",
        "         # English Noun Number\n",
        "         number = morph.get(\"Number\")\n",
        "         if number and isinstance(number, str):\n",
        "             descriptions.append(f\"It is in the **{number}** number.\")\n",
        "\n",
        "    if not descriptions:\n",
        "         descriptions.append(f\"Standard noun form in {lang}.\")\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "def describe_adjective_features_rulebased(token: spacy.tokens.Token, lang: str) -> List[str]:\n",
        "    \"\"\"Describes adjective features and position in user-friendly terms.\"\"\"\n",
        "    descriptions = []\n",
        "    morph = get_morph_features(token)\n",
        "\n",
        "    if lang == 'fr':\n",
        "        # French Adjective Agreement (Gender and Number)\n",
        "        gender = morph.get(\"Gender\")\n",
        "        number = morph.get(\"Number\")\n",
        "        if (gender and isinstance(gender, str)) or (number and isinstance(number, str)):\n",
        "             agreement_info = []\n",
        "             if gender and isinstance(gender, str): agreement_info.append(gender.lower())\n",
        "             if number and isinstance(number, str): agreement_info.append(number.lower())\n",
        "             if agreement_info:\n",
        "                descriptions.append(f\"This adjective is in the **{', '.join(agreement_info)}** form.\")\n",
        "                descriptions.append(\"In French, adjectives must agree in gender and number with the noun they modify.\")\n",
        "\n",
        "    if not descriptions:\n",
        "         descriptions.append(f\"Standard adjective form in {lang}.\")\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "def describe_determiner_features_rulebased(token: spacy.tokens.Token, lang: str) -> List[str]:\n",
        "    \"\"\"Describes determiner (article) usage and features in user-friendly terms.\"\"\"\n",
        "    descriptions = []\n",
        "    morph = get_morph_features(token)\n",
        "\n",
        "    if lang == 'fr':\n",
        "        # French Determiner Agreement (Gender and Number)\n",
        "        gender = morph.get(\"Gender\")\n",
        "        number = morph.get(\"Number\")\n",
        "        if (gender and isinstance(gender, str)) or (number and isinstance(number, str)):\n",
        "             agreement_info = []\n",
        "             if gender and isinstance(gender, str): agreement_info.append(gender.lower())\n",
        "             if number and isinstance(number, str): agreement_info.append(number.lower())\n",
        "             if agreement_info:\n",
        "                descriptions.append(f\"This article is in the **{', '.join(agreement_info)}** form.\")\n",
        "                descriptions.append(\"French articles agree in gender and number with the noun they precede.\")\n",
        "\n",
        "    if not descriptions:\n",
        "         descriptions.append(f\"Standard article usage in {lang}.\")\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "\n",
        "def generate_rulebased_linguistic_explanation(source_doc: spacy.tokens.Doc, target_doc: spacy.tokens.Doc, direction: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a detailed linguistic explanation based on rule-based analysis of spaCy Docs.\n",
        "    Focuses on explaining grammatical choices based on observed features and dependency relations.\n",
        "    This is a highly complex rule-based system.\n",
        "    \"\"\"\n",
        "    explanations = []\n",
        "\n",
        "    src_lang = \"English\" if direction == \"EN->FR\" else \"French\"\n",
        "    tgt_lang = \"French\" if direction == \"EN->FR\" else \"English\"\n",
        "    tgt_lang_code = \"fr\" if direction == \"EN->FR\" else \"en\"\n",
        "    src_lang_code = \"en\" if direction == \"EN->FR\" else \"fr\"\n",
        "\n",
        "\n",
        "    explanations.append(f\"**Understanding Your Translation: {src_lang} → {tgt_lang}**\\n\")\n",
        "    explanations.append(f\"Here's a detailed look at the linguistic transformations in the translation of \\\"{source_doc.text}\\\" to \\\"{target_doc.text}\\\":\\n\")\n",
        "\n",
        "    # --- Verb Analysis and Conjugation ---\n",
        "    tgt_verbs = [token for token in target_doc if token.pos_ == \"VERB\"]\n",
        "    if tgt_verbs:\n",
        "        explanations.append(\"### Verb Conjugation and Form\")\n",
        "        for tgt_verb in tgt_verbs:\n",
        "            explanation_lines = [f\"- The verb '**{tgt_verb.text}**' (root form: '{tgt_verb.lemma_}')\"]\n",
        "\n",
        "            # Describe the verb form and why\n",
        "            verb_form_description = describe_verb_form_rulebased(tgt_verb, tgt_lang_code)\n",
        "            explanation_lines.append(f\"  * It is {verb_form_description}.\")\n",
        "\n",
        "            # Explain agreement based on dependency parse subject\n",
        "            subject_token = None\n",
        "            # Look for a nominal subject (nsubj) or clausal subject (csubj) among children or tokens with verb as head\n",
        "            subject_candidates = [token for token in target_doc if token.head == tgt_verb and token.dep_ in [\"nsubj\", \"csubj\"]]\n",
        "            if subject_candidates:\n",
        "                 subject_token = subject_candidates[0] # Take the first subject found\n",
        "\n",
        "            if subject_token:\n",
        "                 subject_morph = get_morph_features(subject_token)\n",
        "                 subject_person = subject_morph.get('Person', 'N/A')\n",
        "                 subject_number = subject_morph.get('Number', 'N/A')\n",
        "                 verb_morph = get_morph_features(tgt_verb)\n",
        "                 verb_person = verb_morph.get('Person', 'N/A')\n",
        "                 verb_number = verb_morph.get('Number', 'N/A')\n",
        "\n",
        "                 agreement_notes = []\n",
        "                 if subject_person != 'N/A' and verb_person != 'N/A' and subject_person == verb_person:\n",
        "                      agreement_notes.append(f\"agrees in person ({subject_person})\")\n",
        "                 elif subject_person != 'N/A' and verb_person != 'N/A':\n",
        "                      agreement_notes.append(f\"person changed ({subject_person} → {verb_person})\")\n",
        "\n",
        "                 if subject_number != 'N/A' and verb_number != 'N/A' and subject_number == verb_number:\n",
        "                      agreement_notes.append(f\"agrees in number ({subject_number})\")\n",
        "                 elif subject_number != 'N/A' and verb_number != 'N/A':\n",
        "                      agreement_notes.append(f\"number changed ({subject_number} → {verb_number})\")\n",
        "\n",
        "                 if agreement_notes:\n",
        "                     explanation_lines.append(f\"    * In {tgt_lang}, this verb's form {', and '.join(agreement_notes)} with its subject, '**{subject_token.text}**'.\")\n",
        "                 elif tgt_lang_code == 'fr' and ('Person' in verb_morph or 'Number' in verb_morph):\n",
        "                      explanation_lines.append(f\"    * In French, this verb form typically requires agreement in person and number with its subject, which appears to be '**{subject_token.text}**'.\")\n",
        "\n",
        "            elif tgt_lang_code == 'fr' and ('Person' in get_morph_features(tgt_verb) or 'Number' in get_morph_features(tgt_verb)):\n",
        "                 # If French verb is conjugated but subject not found, note agreement is required\n",
        "                 explanation_lines.append(f\"    * In French, this verb form requires agreement in person and number with its subject (subject not explicitly identified by analysis).\")\n",
        "\n",
        "\n",
        "            # Check for common French compound tenses (e.g., Passé Composé, Futur Proche)\n",
        "            if tgt_lang_code == 'fr':\n",
        "                 # Check for Passé Composé (avoir/être + past participle)\n",
        "                 if tgt_verb.lemma_ in ['avoir', 'être'] and tgt_verb.i + 1 < len(target_doc) and target_doc[tgt_verb.i + 1].pos_ == 'VERB' and 'VerbForm=Part' in get_morph_features(target_doc[tgt_verb.i + 1]).values():\n",
        "                      explanation_lines.append(f\"  * **Compound Tense:** Together with the following past participle ('{target_doc[tgt_verb.i + 1].text}'), it forms a compound tense (like the *Passé Composé*), typically used to describe past actions.\")\n",
        "                 # Check for Futur Proche (aller + infinitive)\n",
        "                 elif tgt_verb.lemma_ == 'aller' and tgt_verb.i + 1 < len(target_doc) and target_doc[tgt_verb.i + 1].pos_ == 'VERB' and 'VerbForm=Inf' in get_morph_features(target_doc[tgt_verb.i + 1]).values():\n",
        "                      explanation_lines.append(f\"  * **Compound Tense:** Together with the following infinitive ('{target_doc[tgt_verb.i + 1].text}'), it forms the *Futur Proche* (near future) tense, indicating an action that will happen soon.\")\n",
        "\n",
        "            explanations.append(\"\".join(explanation_lines))\n",
        "\n",
        "    # --- Nouns, Adjectives, and Articles (Focus on Agreement and Usage) ---\n",
        "    if (src_lang == \"English\" and tgt_lang == \"French\") or (src_lang == \"French\" and tgt_lang == \"English\"):\n",
        "        tgt_nouns = [token for token in target_doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
        "        tgt_adjs = [token for token in target_doc if token.pos_ == \"ADJ\"]\n",
        "        tgt_dets = [token for token in target_doc if token.pos_ == \"DET\"]\n",
        "\n",
        "        if tgt_nouns or tgt_adjs or tgt_dets:\n",
        "             explanations.append(\"### Noun, Adjective, and Article Agreement and Usage (in French)\")\n",
        "\n",
        "             # Explain Nouns and their modifiers\n",
        "             explained_adjs = set()\n",
        "             explained_dets = set()\n",
        "\n",
        "             for tgt_noun in tgt_nouns:\n",
        "                 explanation_lines = [f\"- The noun '**{tgt_noun.text}**'\"]\n",
        "                 noun_features = describe_noun_features_rulebased(tgt_noun, tgt_lang)\n",
        "                 if noun_features:\n",
        "                     explanation_lines.append(\"  * \" + \" \".join(noun_features))\n",
        "\n",
        "                 # Find and explain modifying adjectives (using dependency parse and proximity)\n",
        "                 modifying_adjs = [adj for adj in tgt_adjs if adj.head == tgt_noun or (abs(adj.i - tgt_noun.i) <= 3 and adj.dep_ in ['amod', 'acl', 'relcl'])] # Check dependency or proximity+deps\n",
        "                 if modifying_adjs:\n",
        "                     explanation_lines.append(f\"  * It is modified by the adjective(s) **{', '.join([a.text for a in modifying_adjs])}**.\")\n",
        "                     if tgt_lang == \"French\":\n",
        "                          explanation_lines.append(f\"    * In French, these adjectives must agree in **gender** and **number** with the noun '**{tgt_noun.text}**'.\")\n",
        "                          # Add specific adjective agreement details if possible\n",
        "                          for adj in modifying_adjs:\n",
        "                               adj_morph = get_morph_features(adj)\n",
        "                               adj_gender = adj_morph.get('Gender', 'N/A')\n",
        "                               adj_number = adj_morph.get('Number', 'N/A')\n",
        "                               noun_morph = get_morph_features(tgt_noun)\n",
        "                               noun_gender = noun_morph.get('Gender', 'N/A')\n",
        "                               noun_number = noun_morph.get('Number', 'N/A')\n",
        "                               agreement_status = []\n",
        "                               if adj_gender != 'N/A' and noun_gender != 'N/A' and adj_gender == noun_gender: agreement_status.append('gender agreement')\n",
        "                               elif adj_gender != 'N/A' and noun_gender != 'N/A': agreement_status.append(f'gender mismatch ({adj_gender} vs {noun_gender})')\n",
        "                               if adj_number != 'N/A' and noun_number != 'N/A' and adj_number == noun_number: agreement_status.append('number agreement')\n",
        "                               elif adj_number != 'N/A' and noun_number != 'N/A': agreement_status.append(f'number mismatch ({adj_number} vs {noun_number})')\n",
        "                               if agreement_status:\n",
        "                                    explanation_lines.append(f\"      - Adjective '**{adj.text}**': {', '.join(agreement_status)}.\")\n",
        "                               else:\n",
        "                                    explanation_lines.append(f\"      - Adjective '**{adj.text}**': Agreement details not fully determined by analysis.\")\n",
        "\n",
        "                          # Discuss adjective position heuristics in French\n",
        "                          preceding_adjs = [adj for adj in modifying_adjs if adj.i < tgt_noun.i]\n",
        "                          following_adjs = [adj for adj in modifying_adjs if adj.i > tgt_noun.i]\n",
        "                          if preceding_adjs and following_adjs:\n",
        "                              explanation_lines.append(f\"    * Note the position: some adjectives ('{', '.join([a.text for a in preceding_adjs])}') precede the noun, while others ('{', '.join([a.text for a in following_adjs])}') follow it. Adjective position in French depends on the adjective type.\")\n",
        "                          elif preceding_adjs:\n",
        "                               explanation_lines.append(f\"    * Note the position: adjectives ('{', '.join([a.text for a in preceding_adjs])}') precede the noun. This is common for certain types of adjectives (e.g., BAGS adjectives - Beauty, Age, Goodness, Size).\")\n",
        "                          elif following_adjs:\n",
        "                               explanation_lines.append(f\"    * Note the position: adjectives ('{', '.join([a.text for a in following_adjs])}') follow the noun. This is the more common position for many descriptive adjectives in French.\")\n",
        "\n",
        "\n",
        "                          for adj in modifying_adjs: explained_adjs.add(adj)\n",
        "\n",
        "\n",
        "                 # Find and explain modifying determiners (articles) (using dependency parse and proximity)\n",
        "                 modifying_dets = [det for det in tgt_dets if det.head == tgt_noun or (abs(det.i - tgt_noun.i) <= 2 and det.dep_ in ['det'])] # Check dependency or proximity+det\n",
        "                 if modifying_dets:\n",
        "                     explanation_lines.append(f\"  * It uses the article(s) '**{', '.join([d.text for d in modifying_dets])}**'.\")\n",
        "                     if tgt_lang == \"French\":\n",
        "                          explanation_lines.append(f\"    * French articles must also agree in **gender** and **number** with the noun '**{tgt_noun.text}**'.\")\n",
        "                          for det in modifying_dets:\n",
        "                               det_morph = get_morph_features(det)\n",
        "                               det_gender = det_morph.get('Gender', 'N/A')\n",
        "                               det_number = det_morph.get('Number', 'N/A')\n",
        "                               noun_morph = get_morph_features(tgt_noun)\n",
        "                               noun_gender = noun_morph.get('Gender', 'N/A')\n",
        "                               noun_number = noun_morph.get('Number', 'N/A')\n",
        "                               agreement_status = []\n",
        "                               if det_gender != 'N/A' and noun_gender != 'N/A' and det_gender == noun_gender: agreement_status.append('gender agreement')\n",
        "                               elif det_gender != 'N/A' and noun_gender != 'N/A': agreement_status.append(f'gender mismatch ({det_gender} vs {noun_gender})')\n",
        "                               if det_number != 'N/A' and noun_number != 'N/A' and det_number == noun_number: agreement_status.append('number agreement')\n",
        "                               elif det_number != 'N/A' and noun_number != 'N/A': agreement_status.append(f'number mismatch ({det_number} vs {noun_number})')\n",
        "                               if agreement_status:\n",
        "                                    explanation_lines.append(f\"      - Article '**{det.text}**': {', '.join(agreement_status)}.\")\n",
        "                               else:\n",
        "                                    explanation_lines.append(f\"      - Article '**{det.text}**': Agreement details not fully determined by analysis.\")\n",
        "\n",
        "                          # Discuss article usage (definite, indefinite, partitive, contractions)\n",
        "                          for det in modifying_dets:\n",
        "                               det_text_lower = det.text.lower()\n",
        "                               if det_text_lower in ['le', 'la', 'l\\'', 'les']:\n",
        "                                    explanation_lines.append(f\"    * '**{det.text}**' is a definite article, used for specific or known nouns.\")\n",
        "                               elif det_text_lower in ['un', 'une', 'des']:\n",
        "                                    explanation_lines.append(f\"    * '**{det.text}**' is an indefinite article, used for non-specific or unknown nouns.\")\n",
        "                               elif det_text_lower in ['du', 'de la', 'de l\\'', 'des']:\n",
        "                                    explanation_lines.append(f\"    * '**{det.text}**' is a partitive article, used for uncountable nouns or a portion of a countable noun.\")\n",
        "                               elif det_text_lower in ['au', 'aux']:\n",
        "                                    explanation_lines.append(f\"    * '**{det.text}**' is a contraction of 'à le' or 'à les', used with the preposition 'à'.\")\n",
        "                               elif det_text_lower in ['du', 'des']:\n",
        "                                     # Handle du/des as contractions of de + le/les if appropriate\n",
        "                                     if det.head and det.head.lemma_ == 'de': # Simple check if head is 'de'\n",
        "                                          explanation_lines.append(f\"    * '**{det.text}**' can also be a contraction of 'de le' or 'de les', used with the preposition 'de'.\")\n",
        "\n",
        "\n",
        "                          for det in modifying_dets: explained_dets.add(det)\n",
        "\n",
        "                 explanations.append(\"\".join(explanation_lines))\n",
        "\n",
        "             # Add explanations for adjectives/determiners not linked to a noun above\n",
        "             unlinked_adjs = [adj for adj in tgt_adjs if adj not in explained_adjs]\n",
        "             if unlinked_adjs:\n",
        "                  explanations.append(f\"- The translation also includes the {tgt_lang} adjective(s) **{', '.join([a.text for a in unlinked_adjs])}**.\")\n",
        "                  if tgt_lang == \"French\":\n",
        "                       explanations.append(\"  * These adjectives also need to agree with the noun they modify (even if not explicitly linked by the analysis tool). Adjective position relative to nouns can differ between languages.\")\n",
        "\n",
        "             unlinked_dets = [det for det in tgt_dets if det not in explained_dets]\n",
        "             if unlinked_dets:\n",
        "                  explanations.append(f\"- The translation uses {tgt_lang} articles like **{', '.join([d.text for d in unlinked_dets])}**.\")\n",
        "                  if tgt_lang == \"French\":\n",
        "                       explanations.append(\"  * French articles agree in gender and number with the noun they precede. Article usage varies between languages.\")\n",
        "\n",
        "\n",
        "    # --- Word Order and Structural Differences ---\n",
        "    explanations.append(\"### Word Order and Sentence Structure\")\n",
        "    explanations.append(f\"- The arrangement of words in the sentence is adjusted to follow typical {tgt_lang} sentence structure. This can be different from {src_lang} word order.\")\n",
        "    # Add more specific examples here by comparing dependency parse structures, though this is complex.\n",
        "    # Example: Identify the subject, verb, and object in both sentences and comment on their order.\n",
        "    # This requires robust alignment, which is difficult rule-based.\n",
        "    # A simpler approach: comment on common differences like adjective position (already done above but can be reiterated).\n",
        "\n",
        "\n",
        "    # --- Other Notable Transformations (Prepositions, Adverbs, etc.) ---\n",
        "    explanations.append(\"### Other Transformations\")\n",
        "    # Identify prepositions and their heads/children to see how phrases are constructed\n",
        "    tgt_preps = [token for token in target_doc if token.pos_ == \"ADP\"]\n",
        "    if tgt_preps:\n",
        "         explanation_lines = [f\"- The translation uses prepositions like **{', '.join([p.text for p in tgt_preps])}**.\"]\n",
        "         if tgt_lang == \"French\":\n",
        "              explanation_lines.append(\" In French, prepositions are crucial for indicating relationships between words and phrases (e.g., location, time, possession).\")\n",
        "              # Could add examples of prepositional phrases if identifiable\n",
        "         explanations.append(\"\".join(explanation_lines))\n",
        "    else:\n",
        "         explanations.append(\"- Prepositions were used to build phrases in the translation.\")\n",
        "\n",
        "\n",
        "    tgt_advs = [token for token in target_doc if token.pos_ == \"ADV\"]\n",
        "    if tgt_advs:\n",
        "         explanation_lines = [f\"- Adverbs like **{', '.join([a.text for a in tgt_advs])}** are used to modify verbs, adjectives, or other adverbs.\"]\n",
        "         # Could comment on their position if it's notably different from English\n",
        "         explanations.append(\"\".join(explanation_lines))\n",
        "\n",
        "\n",
        "    # --- General Note ---\n",
        "    explanations.append(\"### How This Translation Was Generated\")\n",
        "    explanations.append(f\"- This translation was produced by a neural machine translation model. The model learned to make these linguistic transformations by analyzing millions of example sentences. It doesn't apply explicit grammatical rules like a human, but rather recognizes patterns to generate grammatically correct and natural-sounding translations in {tgt_lang}.\")\n",
        "\n",
        "\n",
        "    return \"\\n\".join(explanations)\n",
        "\n",
        "\n",
        "# --- Flask Routes ---\n",
        "\n",
        "# HTML Template (embedded as a string for Colab simplicity)\n",
        "# In a real app, save this as templates/index.html and use render_template\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Explainable Translator (EN-FR)</title>\n",
        "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Inter', sans-serif;\n",
        "        }\n",
        "        /* Custom styles for potential enhancements */\n",
        "        .explanation-section {\n",
        "            margin-top: 1.5rem;\n",
        "            padding-top: 1.5rem;\n",
        "            border-top: 1px solid #e5e7eb; /* Tailwind gray-200 */\n",
        "        }\n",
        "        .explanation-section h3 {\n",
        "            font-size: 1.25rem; /* Tailwind text-xl */\n",
        "            font-weight: 600; /* Tailwind font-semibold */\n",
        "            margin-bottom: 0.75rem; /* Tailwind mb-3 */\n",
        "        }\n",
        "        /* Style for Markdown output */\n",
        "        #explanationOutput h3 {\n",
        "            font-size: 1.25rem;\n",
        "            font-weight: 600;\n",
        "            margin-top: 1.5rem;\n",
        "            margin-bottom: 0.75rem;\n",
        "            border-bottom: 1px solid #d1d5db; /* Tailwind gray-300 */\n",
        "            padding-bottom: 0.25rem;\n",
        "        }\n",
        "        #explanationOutput ul {\n",
        "            list-style-type: disc;\n",
        "            margin-left: 1.25rem;\n",
        "            margin-bottom: 1rem;\n",
        "        }\n",
        "         #explanationOutput li {\n",
        "            margin-bottom: 0.5rem;\n",
        "         }\n",
        "        #explanationOutput strong {\n",
        "            font-weight: 700; /* Tailwind font-bold */\n",
        "        }\n",
        "         #explanationOutput em {\n",
        "            font-style: italic;\n",
        "         }\n",
        "         /* Style for scrollable explanation area */\n",
        "        #explanationOutput {\n",
        "            overflow-y: auto;\n",
        "            min-height: 200px; /* Ensure a minimum height */\n",
        "            max-height: 400px; /* Set a maximum height with scrolling */\n",
        "            border: 1px solid #d1d5db; /* Add a border */\n",
        "            border-radius: 0.375rem; /* Add rounded corners */\n",
        "        }\n",
        "         /* Style for text areas */\n",
        "        textarea {\n",
        "             resize: vertical; /* Allow vertical resizing */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body class=\"bg-gray-100 text-gray-900 flex items-center justify-center min-h-screen p-4\">\n",
        "    <div class=\"container mx-auto bg-white shadow-xl rounded-xl p-8 max-w-4xl w-full\">\n",
        "\n",
        "        <h1 class=\"text-3xl font-bold text-center text-gray-800 mb-6\">\n",
        "            ✨ Explainable Translator (EN↔️FR) ✨\n",
        "        </h1>\n",
        "        <p class=\"text-center text-gray-600 mb-8\">\n",
        "            Translate text between English and French and get a detailed linguistic explanation.\n",
        "        </p>\n",
        "\n",
        "        <div class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n",
        "\n",
        "            <div class=\"flex flex-col\">\n",
        "                <label for=\"inputText\" class=\"block text-sm font-medium text-gray-700 mb-2\">Enter Text (English or French)</label>\n",
        "                <textarea id=\"inputText\" rows=\"6\" class=\"shadow-sm focus:ring-blue-500 focus:border-blue-500 block w-full sm:text-sm border-gray-300 rounded-md p-3\" placeholder=\"Type your sentence here...\"></textarea>\n",
        "\n",
        "                <label for=\"direction\" class=\"block text-sm font-medium text-gray-700 mt-4 mb-2\">Translation Direction</label>\n",
        "                <select id=\"direction\" class=\"mt-1 block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md shadow-sm\">\n",
        "                    <option value=\"EN->FR\">English → French</option>\n",
        "                    <option value=\"FR->EN\">French → English</option>\n",
        "                </select>\n",
        "\n",
        "                <button id=\"translateButton\" class=\"mt-6 w-full inline-flex items-center justify-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-blue-600 hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition duration-150 ease-in-out\">\n",
        "                    Translate and Explain ✨\n",
        "                </button>\n",
        "            </div>\n",
        "\n",
        "            <div class=\"flex flex-col\">\n",
        "                <label for=\"translationOutput\" class=\"block text-sm font-medium text-gray-700 mb-2\">Translation Output</label>\n",
        "                <textarea id=\"translationOutput\" rows=\"6\" class=\"shadow-sm block w-full sm:text-sm border-gray-300 rounded-md p-3 bg-gray-50\" readonly placeholder=\"Translation will appear here...\"></textarea>\n",
        "\n",
        "                <label for=\"explanationOutput\" class=\"block text-sm font-medium text-gray-700 mt-4 mb-2\">Linguistic Explanation</label>\n",
        "                <div id=\"explanationOutput\" class=\"shadow-sm block w-full sm:text-sm border-gray-300 rounded-md p-4 bg-gray-50\" style=\"min-height: 200px; max-height: 400px;\">\n",
        "                    <p class=\"text-gray-500\">Explanation will appear here after translation.</p>\n",
        "                </div>\n",
        "                 <div id=\"errorMessage\" class=\"mt-4 text-red-600 text-sm hidden\"></div>\n",
        "            </div>\n",
        "\n",
        "        </div>\n",
        "\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        document.getElementById('translateButton').addEventListener('click', async () => {\n",
        "            const inputText = document.getElementById('inputText').value;\n",
        "            const direction = document.getElementById('direction').value;\n",
        "            const translationOutput = document.getElementById('translationOutput');\n",
        "            const explanationOutput = document.getElementById('explanationOutput');\n",
        "            const errorMessage = document.getElementById('errorMessage');\n",
        "\n",
        "            // Clear previous outputs and errors\n",
        "            translationOutput.value = '';\n",
        "            explanationOutput.innerHTML = '<p class=\"text-gray-500\">Translating and analyzing...</p>';\n",
        "            errorMessage.classList.add('hidden');\n",
        "            errorMessage.textContent = '';\n",
        "\n",
        "            // Show loading indicator on button\n",
        "            const translateButton = document.getElementById('translateButton');\n",
        "            const originalButtonText = translateButton.innerHTML;\n",
        "            translateButton.disabled = true;\n",
        "            translateButton.innerHTML = `\n",
        "                <svg class=\"animate-spin -ml-1 mr-3 h-5 w-5 text-white\" xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 24 24\">\n",
        "                    <circle class=\"opacity-25\" cx=\"12\" cy=\"12\" r=\"10\" stroke=\"currentColor\" stroke-width=\"4\"></circle>\n",
        "                    <path class=\"opacity-75\" fill=\"currentColor\" d=\"M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z\"></path>\n",
        "                </svg>\n",
        "                Processing...\n",
        "            `;\n",
        "\n",
        "\n",
        "            try {\n",
        "                // --- Actual Fetch Call to Flask Backend ---\n",
        "                const response = await fetch('/translate_explain', { // Endpoint defined in Flask app\n",
        "                    method: 'POST',\n",
        "                    headers: {\n",
        "                        'Content-Type': 'application/json',\n",
        "                    },\n",
        "                    body: JSON.stringify({ source_text: inputText, direction: direction }),\n",
        "                });\n",
        "\n",
        "                if (!response.ok) {\n",
        "                    // Handle HTTP errors (e.g., 404, 500)\n",
        "                    throw new Error(`HTTP error! status: ${response.status}`);\n",
        "                }\n",
        "\n",
        "                const result = await response.json(); // Assuming backend returns JSON\n",
        "\n",
        "                if (result.status === 'success' || result.status === 'warning') {\n",
        "                    translationOutput.value = result.translation;\n",
        "                    // The explanation is expected to be Markdown/HTML\n",
        "                    explanationOutput.innerHTML = result.explanation;\n",
        "                    if (result.status === 'warning') {\n",
        "                         errorMessage.classList.remove('hidden');\n",
        "                         errorMessage.textContent = `Warning: ${result.error}`;\n",
        "                    } else {\n",
        "                         errorMessage.classList.add('hidden');\n",
        "                         errorMessage.textContent = '';\n",
        "                    }\n",
        "                } else {\n",
        "                    // Handle backend-specific errors\n",
        "                    errorMessage.classList.remove('hidden');\n",
        "                    errorMessage.textContent = `Error: ${result.error || 'An unknown error occurred on the backend.'}`;\n",
        "                    explanationOutput.innerHTML = `<p class=\"text-red-600\">${result.error || 'Translation failed.'}</p>`;\n",
        "                }\n",
        "\n",
        "            } catch (error) {\n",
        "                // Handle network errors or errors during fetch processing\n",
        "                console.error('Fetch error:', error);\n",
        "                errorMessage.classList.remove('hidden');\n",
        "                errorMessage.textContent = `Request failed: ${error}`;\n",
        "                explanationOutput.innerHTML = `<p class=\"text-red-600\">Could not connect to the translation service. Please ensure the backend is running.</p>`;\n",
        "            } finally {\n",
        "                 // Restore button state\n",
        "                 translateButton.disabled = false;\n",
        "                 translateButton.innerHTML = originalButtonText;\n",
        "            }\n",
        "\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# --- Flask Routes ---\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    \"\"\"Renders the main HTML page.\"\"\"\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/translate_explain', methods=['POST'])\n",
        "def translate_and_explain_route():\n",
        "    \"\"\"Handles translation and explanation requests from the frontend.\"\"\"\n",
        "    # Check if essential libraries were loaded before processing\n",
        "    if not essential_libraries_loaded:\n",
        "        return jsonify({\n",
        "            \"translation\": \"\",\n",
        "            \"explanation\": \"**Initialization Error:** Required libraries or spaCy models failed to load. Please check the Colab output for installation instructions.\",\n",
        "            \"status\": \"error\",\n",
        "            \"error\": \"Initialization failed.\"\n",
        "        }), 500 # Return a 500 Internal Server Error status\n",
        "\n",
        "    data = request.get_json()\n",
        "    source_text = data.get('source_text', '')\n",
        "    direction = data.get('direction', 'EN->FR') # Default direction\n",
        "\n",
        "    # Call the backend logic function\n",
        "    result = get_translation_and_explanation(source_text, direction)\n",
        "\n",
        "    # Return the result as JSON\n",
        "    return jsonify(result)\n",
        "\n",
        "\n",
        "# --- Backend Logic Function (Called by Flask Route) ---\n",
        "def get_translation_and_explanation(source_text: str, direction: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs translation and generates a rule-based linguistic explanation.\n",
        "    Returns a dictionary suitable for a JSON response.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Received Request (Backend Logic) ---\")\n",
        "    print(f\"Direction: {direction}\")\n",
        "    print(f\"Input Text: '{source_text[:100]}...'\") # Print first 100 chars\n",
        "\n",
        "    # Initialize outputs\n",
        "    translation_output = \"\"\n",
        "    textual_explanation = \"Processing...\"\n",
        "    status = \"success\"\n",
        "    error_message = None\n",
        "\n",
        "    # Input validation\n",
        "    if not source_text.strip():\n",
        "        status = \"error\"\n",
        "        error_message = \"Please enter some text to translate and analyze.\"\n",
        "        textual_explanation = error_message # Also set explanation for clarity\n",
        "        return {\"translation\": translation_output, \"explanation\": textual_explanation, \"status\": status, \"error\": error_message}\n",
        "\n",
        "    if direction not in [\"EN->FR\", \"FR->EN\"]:\n",
        "        status = \"error\"\n",
        "        error_message = \"Invalid translation direction selected.\"\n",
        "        textual_explanation = error_message # Also set explanation for clarity\n",
        "        return {\"translation\": translation_output, \"explanation\": textual_explanation, \"status\": status, \"error\": error_message}\n",
        "\n",
        "    source_lang_code = \"en\" if direction == \"EN->FR\" else \"fr\"\n",
        "    target_lang_code = \"fr\" if direction == \"EN->FR\" else \"en\" # Corrected target lang code logic\n",
        "    source_lang_name = \"English\" if direction == \"EN->FR\" else \"French\"\n",
        "    target_lang_name = \"French\" if direction == \"EN->FR\" else \"English\"\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    translation_successful = False\n",
        "    current_translation_output = \"\"\n",
        "\n",
        "    try:\n",
        "        # 1. Load appropriate model and tokenizer\n",
        "        # Check if transformers and torch are available before loading model\n",
        "        if 'transformers' not in sys.modules or 'torch' not in sys.modules:\n",
        "             raise RuntimeError(\"Translation libraries (transformers/torch) not loaded.\")\n",
        "\n",
        "        model_name = MODEL_EN_FR if direction == \"EN->FR\" else MODEL_FR_EN\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "        device = get_device() # Ensure device is known\n",
        "\n",
        "        # 2. Prepare input\n",
        "        # Add language code prefix for Helsinki-NLP models\n",
        "        # See https://huggingface.co/Helsinki-NLP/opus-mt-en-fr#translation-example\n",
        "        if direction == \"FR->EN\":\n",
        "             # For FR->EN, the source text needs the >>en<< prefix\n",
        "             input_text_processed = f\">>en<< {source_text}\"\n",
        "        else: # EN->FR\n",
        "             # For EN->FR, the source text needs the >>fr<< prefix\n",
        "             input_text_processed = f\">>fr<< {source_text}\"\n",
        "\n",
        "        inputs = tokenizer(input_text_processed, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # 3. Generate translation\n",
        "        print(\"Generating translation...\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=512,\n",
        "                num_beams=4, # Use beam search (common default for generation)\n",
        "                early_stopping=True,\n",
        "                output_scores=False,\n",
        "                return_dict_in_generate=True\n",
        "            )\n",
        "\n",
        "        # 4. Decode generated sequence\n",
        "        generated_ids = outputs.sequences[0]\n",
        "        current_translation_output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "        print(f\"Translation: {current_translation_output}\")\n",
        "        translation_successful = True\n",
        "\n",
        "    except RuntimeError as e:\n",
        "         print(f\"Runtime Error during translation: {e}\")\n",
        "         status = \"error\"\n",
        "         error_message = f\"Error during translation: {e}\"\n",
        "         current_translation_output = \"\" # Clear output on error\n",
        "         textual_explanation = error_message\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during translation: {e}\")\n",
        "        status = \"error\"\n",
        "        error_message = f\"An unexpected error occurred during translation: {e}\"\n",
        "        current_translation_output = \"\" # Clear output on error\n",
        "        textual_explanation = error_message\n",
        "\n",
        "\n",
        "    # 5. Generate Detailed Linguistic Explanation (only if translation was successful)\n",
        "    if translation_successful:\n",
        "        print(\"Performing linguistic analysis and generating explanation...\")\n",
        "        # Check if spaCy models are available before analyzing\n",
        "        if not ('spacy' in sys.modules and nlp_en and nlp_fr):\n",
        "             status = \"warning\" # Analysis failed, but translation might be OK\n",
        "             error_message = \"Linguistic analysis libraries (spaCy) or models not loaded. Explanation will be unavailable.\"\n",
        "             textual_explanation = error_message # Provide a message in the explanation field\n",
        "        else:\n",
        "            source_doc = analyze_sentence(source_text, source_lang_code)\n",
        "            target_doc = analyze_sentence(current_translation_output, target_lang_code)\n",
        "\n",
        "            if source_doc is None or target_doc is None:\n",
        "                 status = \"warning\" # Analysis failed, but translation might be OK\n",
        "                 error_message = \"Could not perform detailed linguistic analysis (spaCy models not loaded). Explanation will be unavailable.\"\n",
        "                 textual_explanation = error_message # Provide a message in the explanation field\n",
        "            else:\n",
        "                # Generate explanation using rule-based logic\n",
        "                textual_explanation = generate_rulebased_linguistic_explanation(\n",
        "                    source_doc,\n",
        "                    target_doc,\n",
        "                    direction\n",
        "                )\n",
        "                print(\"Rule-based linguistic explanation generated.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"--- Request processed in {end_time - start_time:.2f} seconds ---\")\n",
        "    clean_memory() # Clean up memory after processing\n",
        "\n",
        "    # Return results as a dictionary\n",
        "    return {\n",
        "        \"translation\": current_translation_output,\n",
        "        \"explanation\": textual_explanation,\n",
        "        \"status\": status,\n",
        "        \"error\": error_message\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Running the Flask App (for Colab) ---\n",
        "# This block allows you to run the Flask app directly within a Colab cell.\n",
        "# You might need to expose the port using ngrok or colab_tunnel for public access.\n",
        "if __name__ == '__main__':\n",
        "    # Only attempt to run the app if essential libraries were loaded\n",
        "    if not essential_libraries_loaded:\n",
        "        print(\"\\nFlask app cannot start due to missing essential libraries.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Use pyngrok to expose the port\n",
        "            from pyngrok import ngrok\n",
        "            import os # Ensure os is imported here for environment variable access\n",
        "\n",
        "            # Terminate any existing ngrok tunnels\n",
        "            ngrok.kill()\n",
        "\n",
        "            # Get ngrok authtoken from environment variable\n",
        "            NGROK_AUTH_TOKEN = os.environ[\"NGROK_AUTH_TOKEN\"] =\n",
        "\n",
        "            if NGROK_AUTH_TOKEN:\n",
        "                # Set the authtoken\n",
        "                ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "                print(\"ngrok authtoken set.\")\n",
        "                # Start a new ngrok HTTP tunnel on port 5000\n",
        "                public_url = ngrok.connect(5000)\n",
        "                print(f\"\\n🎉 Flask app is running! Access it at: {public_url}\\n\")\n",
        "\n",
        "                print(\"Starting Flask app...\")\n",
        "                # Set debug=True for development, False for production\n",
        "                # host='0.0.0.0' makes the server accessible externally (needed for ngrok/colab_tunnel)\n",
        "                # port=5000 is a common default Flask port\n",
        "                app.run(host='0.0.0.0', port=5000, debug=True)\n",
        "                print(\"Flask app stopped.\")\n",
        "            else:\n",
        "                print(\"\\nNGROK_AUTH_TOKEN environment variable not set.\")\n",
        "                print(\"Please get your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "                print(\"And set it in Colab using: import os; os.environ['NGROK_AUTH_TOKEN'] = 'YOUR_AUTH_TOKEN'\")\n",
        "                print(\"Flask app will run locally but won't be publicly accessible without ngrok.\")\n",
        "                # Run locally if ngrok authtoken is not set\n",
        "                app.run(host='0.0.0.0', port=5000, debug=True)\n",
        "\n",
        "\n",
        "        except ImportError:\n",
        "             print(\"\\nCould not import pyngrok. Please install it using: !pip install pyngrok\")\n",
        "             print(\"Flask app will run locally but won't be publicly accessible without a tunnel.\")\n",
        "             # Fallback to running without tunnel if pyngrok is not available\n",
        "             app.run(host='0.0.0.0', port=5000, debug=True)\n",
        "        except Exception as e:\n",
        "             print(f\"\\nAn error occurred while starting the Flask app or pyngrok: {e}\")\n",
        "             print(\"Please check the error message and ensure the port (5000) is not already in use.\")\n",
        "             print(\"Also ensure your ngrok authtoken is correctly set.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AyzPVYRkT55l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff35d8f9-46f9-4fea-af40-df9576373c33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing libraries...\n",
            "spaCy models already installed.\n",
            "Required libraries (Flask, Torch, Transformers, spaCy, pyngrok) imported successfully.\n",
            "Flask app initialized.\n",
            "ngrok authtoken set.\n",
            "\n",
            "🎉 Flask app is running! Access it at: NgrokTunnel: \"https://3c7e-34-169-155-92.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "\n",
            "Starting Flask app...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app stopped.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}